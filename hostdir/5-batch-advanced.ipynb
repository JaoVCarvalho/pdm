{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2665f558-53d8-4586-bfa4-43830aa5384f",
   "metadata": {},
   "source": [
    "# EX5-BATCH: More advanced RDD API programming\n",
    "\n",
    "Your assignment: complete the `TODO`'s and include also the **output of each cell**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7db17110-5cf4-44f8-b34d-64d7b91cd768",
   "metadata": {},
   "source": [
    "### Download Bike Trip Data (Feb 2025)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ede39833-c55e-448d-8c3e-8d0fc1b5e8cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to s3.amazonaws.com (52.217.123.56:443)\n",
      "wget: can't open 'data/202502-citibike-tripdata.zip': File exists\n"
     ]
    }
   ],
   "source": [
    "!wget -np https://s3.amazonaws.com/tripdata/202502-citibike-tripdata.zip -P data/\n",
    "![ -e \"data/202502-citibike-tripdata_1.csv\" ] || (cd data/ && unzip 202502-citibike-tripdata.zip)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e7324ec-415f-4e3d-8bc8-539345a90867",
   "metadata": {},
   "source": [
    "### Data is on three files, let us take a look on one (header + a few lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d225bfd7-52b4-4c44-8268-21a131370fbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ride_id,rideable_type,started_at,ended_at,start_station_name,start_station_id,end_station_name,end_station_id,start_lat,start_lng,end_lat,end_lng,member_casual\n",
      "C1F868EC9F7E49A5,electric_bike,2025-02-06 16:54:02.517,2025-02-06 17:00:48.166,Perry St & Bleecker St,5922.07,Watts St & Greenwich St,5578.02,40.73535398,-74.00483091,40.72405549,-74.00965965,member\n",
      "668DDE0CFA929D5A,electric_bike,2025-02-14 10:09:49.035,2025-02-14 10:21:57.856,Dock 72 Way & Market St,4804.02,Spruce St & Nassau St,5137.10,40.69985,-73.97141,40.71146364,-74.00552427,member\n"
     ]
    }
   ],
   "source": [
    "!head -3 data/202502-citibike-tripdata_1.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd13bca3-6dd6-4215-bebf-8b9f3a6a2dc8",
   "metadata": {},
   "source": [
    "### **Dataset Description**\n",
    "The dataset contains **bike trip records** with the following columns:\n",
    "\n",
    "| Column Name            | Description |\n",
    "|------------------------|-------------|\n",
    "| `ride_id`             | Unique trip identifier |\n",
    "| `rideable_type`       | Type of bike used (e.g., docked, electric) |\n",
    "| `started_at`          | Start timestamp of the trip |\n",
    "| `ended_at`            | End timestamp of the trip |\n",
    "| `start_station_name`  | Name of the start station |\n",
    "| `start_station_id`    | ID of the start station |\n",
    "| `end_station_name`    | Name of the end station |\n",
    "| `end_station_id`      | ID of the end station |\n",
    "| `start_lat`          | Latitude of the start location |\n",
    "| `start_lng`          | Longitude of the start location |\n",
    "| `end_lat`            | Latitude of the end location |\n",
    "| `end_lng`            | Longitude of the end location |\n",
    "| `member_casual`       | User type (`member` for subscribers, `casual` for non-subscribers) |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3213ac02-29c9-4818-b516-70e4bdf3cf43",
   "metadata": {},
   "source": [
    "### Step 1: Load and Preprocess the Data\n",
    "1. Start a **PySpark session (or SparkContext)**.\n",
    "2. Load the dataset as an **RDD**.\n",
    "3. **Remove the header** and filter out malformed rows.\n",
    "4. `#TODO` Do the same for each file. Use [Spark Union transformation function](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.union.html) for that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4abbe0bc-4bce-4258-b693-fd9e15d45892",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['C1F868EC9F7E49A5',\n",
       "  'electric_bike',\n",
       "  '2025-02-06 16:54:02.517',\n",
       "  '2025-02-06 17:00:48.166',\n",
       "  'Perry St & Bleecker St',\n",
       "  '5922.07',\n",
       "  'Watts St & Greenwich St',\n",
       "  '5578.02',\n",
       "  '40.73535398',\n",
       "  '-74.00483091',\n",
       "  '40.72405549',\n",
       "  '-74.00965965',\n",
       "  'member'],\n",
       " ['668DDE0CFA929D5A',\n",
       "  'electric_bike',\n",
       "  '2025-02-14 10:09:49.035',\n",
       "  '2025-02-14 10:21:57.856',\n",
       "  'Dock 72 Way & Market St',\n",
       "  '4804.02',\n",
       "  'Spruce St & Nassau St',\n",
       "  '5137.10',\n",
       "  '40.69985',\n",
       "  '-73.97141',\n",
       "  '40.71146364',\n",
       "  '-74.00552427',\n",
       "  'member']]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "\n",
    "try:\n",
    "    sc.stop()\n",
    "except NameError:\n",
    "    print(\"SparkContext not defined\")\n",
    "\n",
    "# Initialize Spark Context\n",
    "sc = SparkContext(appName=\"EX5-BIGDATA\", master=\"local[*]\") # local execution\n",
    "# sc = SparkContext(appName=\"EX5-BIGDATA\", master=\"spark://spark:7077\") # cluster execution\n",
    "\n",
    "# Load data\n",
    "file_path = \"data/202502-citibike-tripdata_1.csv\"\n",
    "raw_rdd = sc.textFile(file_path)\n",
    "\n",
    "# Remove header\n",
    "header = raw_rdd.first()\n",
    "data_rdd = raw_rdd.filter(lambda row: row != header)\n",
    "\n",
    "# Split CSV rows into lists\n",
    "rdd = data_rdd.map(lambda row: row.split(\",\"))\n",
    "\n",
    "# Filter out malformed rows (should have 13 columns)\n",
    "valid_rdd = rdd.filter(lambda cols: len(cols) == 13)\n",
    "\n",
    "valid_rdd.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ebb4a5d-618a-441a-8eac-848b010fd78e",
   "metadata": {},
   "source": [
    "### Step 2: RDD Partitioning\n",
    "1. Check the **initial number of partitions**.\n",
    "2. Repartition the data for better performance (change the number at will).\n",
    "3. See what happens in the Spark UI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "fd73fc87-1758-40f2-906f-5c53d77f9149",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Partitions: 6\n"
     ]
    }
   ],
   "source": [
    "# check initial partitions\n",
    "initial_partitions = valid_rdd.getNumPartitions()\n",
    "print(f\"Initial Partitions: {initial_partitions}\")\n",
    "\n",
    "# change the number of partitions (this will trigger a full shuffle, to reorganize data)\n",
    "partitioned_rdd = valid_rdd.repartition(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "1c4f7863-e3cf-4bcd-b886-c9780b7d23ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10) MapPartitionsRDD[30] at coalesce at NativeMethodAccessorImpl.java:0 []\n",
      " |   CoalescedRDD[29] at coalesce at NativeMethodAccessorImpl.java:0 []\n",
      " |   ShuffledRDD[28] at coalesce at NativeMethodAccessorImpl.java:0 []\n",
      " +-(6) MapPartitionsRDD[27] at coalesce at NativeMethodAccessorImpl.java:0 []\n",
      "    |  PythonRDD[26] at RDD at PythonRDD.scala:53 []\n",
      "    |  data/202502-citibike-tripdata_1.csv MapPartitionsRDD[1] at textFile at NativeMethodAccessorImpl.java:0 []\n",
      "    |  data/202502-citibike-tripdata_1.csv HadoopRDD[0] at textFile at NativeMethodAccessorImpl.java:0 []\n"
     ]
    }
   ],
   "source": [
    "print(partitioned_rdd.toDebugString().decode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f892d3-68dc-4a0d-90df-a9de54ccc69b",
   "metadata": {},
   "source": [
    "### Step 3: Get the top-3 most Popular starting stations\n",
    "1. You should get this information and collect to the drive (tip: function [PySpark RDD sortBy](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.sortBy.html), however, it can be more efficient than that by using the [Reduce Action](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.reduce.html) -- not to be confused with the [ReduceByKey Transformation](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.reduceByKey.html))\n",
    "2. Broadcast this information\n",
    "3. Use the broacast to append to each RDD item a new value: `starting_station_top3`, with values `yes` or `no`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2d3ce869-e755-4a7b-9b7e-17ffe360051e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c76d81b6-a189-43ea-b1b5-e07d69376a3a",
   "metadata": {},
   "source": [
    "### Step 4: Use Accumulators for Data Statistics\n",
    "1. Generate:\n",
    "   - Total trips\n",
    "   - Trips with missing data\n",
    "   - Trips by casual riders vs. members"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9540a469-86ed-4f75-8edd-247279a4e7b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accumulators for statistics\n",
    "total_trips = sc.accumulator(0)\n",
    "invalid_trips = sc.accumulator(0)\n",
    "casual_trips = sc.accumulator(0)\n",
    "member_trips = sc.accumulator(0)\n",
    "\n",
    "# TODO ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda70c25-f23b-440d-b9c4-101511b8e4e7",
   "metadata": {},
   "source": [
    "### Step 5: Other Insights\n",
    "1. Average trip duration for members vs. casual riders.\n",
    "2. Peak riding hours, i.e., the day hour in which more people are riding bikes.\n",
    "\n",
    "Tip: use `datetime` to format string dates and calculate duration, among other date data manipulations. An example below:\n",
    "\n",
    "```\n",
    "start_str = '2025-02-06 16:54:02.517'\n",
    "end_str = '2025-02-06 17:00:48.166'\n",
    "start_time = datetime.strptime(cols[2], \"%Y-%m-%d %H:%M:%S\")\n",
    "end_time = datetime.strptime(cols[3], \"%Y-%m-%d %H:%M:%S\")\n",
    "duration = (end_time - start_time).total_seconds() / 60  # Convert to minutes\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c0b1675c-0fa0-43a0-9b1f-50d98a06b445",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
