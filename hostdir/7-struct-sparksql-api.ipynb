{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2665f558-53d8-4586-bfa4-43830aa5384f",
   "metadata": {},
   "source": [
    "# EX7-STRUCT: SparkSQL aggregates, execution plan, and others\n",
    "\n",
    "Your assignment: complete the `TODO`'s and include also the **output of each cell**.\n",
    "\n",
    "#### You may need to read the [Dataframe API Documentation](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/dataframe.html) to complete this lab."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7db17110-5cf4-44f8-b34d-64d7b91cd768",
   "metadata": {},
   "source": [
    "### Step 1: Download Bike Trip Data (GDrive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede39833-c55e-448d-8c3e-8d0fc1b5e8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gdown\n",
    "\n",
    "# stations data\n",
    "if not os.path.isfile('data/station-data.json'):\n",
    "    id = \"134kLURYaTZuj6SWrg1XvkQKQ30bf1X7I\"\n",
    "    gdown.download(id=id, output=\"data/\")\n",
    "    print(\"Stations data downloaded\")\n",
    "else:\n",
    "    print(\"Stations data already downloaded\")\n",
    "\n",
    "# trips data\n",
    "if not os.path.isfile('data/trip-data.json'):\n",
    "    id = \"1pX3WHi3R2n52zyo6swXxlOtVTgt_hOQG\"\n",
    "    gdown.download(id=id, output=\"data/\")\n",
    "    print(\"Trips data downloaded\")\n",
    "else:\n",
    "    print(\"Trips data already downloaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d78793a7-6d38-4794-a7d1-6b2c0e57ca8f",
   "metadata": {},
   "source": [
    "### Step 2: Start Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6991433e-339e-4dde-a08d-d31ce0c45a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "try:\n",
    "    spark.stop()\n",
    "except NameError:\n",
    "    print(\"SparkContext not defined\")\n",
    "\n",
    "# local mode\n",
    "spark = SparkSession.builder \\\n",
    "            .appName(\"Spark SQL basic example\") \\\n",
    "            .master(\"local[*]\") \\\n",
    "\t    \t.config(\"spark.some.config.option\", \"some-value\") \\\n",
    "\t    \t.getOrCreate()\n",
    "\n",
    "# cluster mode\n",
    "#spark = SparkSession.builder \\\n",
    "#            .appName(\"Spark SQL basic example\") \\\n",
    "#            .master(\"spark://spark:7077\") \\\n",
    "#\t    \t.config(\"spark.some.config.option\", \"some-value\") \\\n",
    "#\t    \t.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c14d39b9-8cb1-4605-9da6-d98d169b9790",
   "metadata": {},
   "source": [
    "### Step 3: Read JSON files as dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d2a9a2-4ec7-4ccf-b0f8-b34a0c30d846",
   "metadata": {},
   "outputs": [],
   "source": [
    "stations_df = spark.read.json('data/station-data.json')\n",
    "stations_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f9fc205-2c22-44b1-a0b2-538516fe39f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "trips_df = spark.read.json('data/trip-data.json').repartition(9)\n",
    "trips_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0007319-3c00-453f-a229-8f341034cbd5",
   "metadata": {},
   "source": [
    "### Step 4: Follow the [groupBy tutorial on SparkByExample](https://sparkbyexamples.com/pyspark/pyspark-groupby-explained-with-example/) `#TODO`\n",
    "1. Reproduce in this notebook each example on the tutorial page, **however, using the stations and trips dataframes instead**\n",
    "2. The objective is for you to take practice on each operator and hence, you may choose the specifics -- column, or columns, queries, etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "043488f1-13b6-444a-b808-657f986f8679",
   "metadata": {},
   "outputs": [],
   "source": [
    "# .. TODO, multiple tutorial steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f97ca4ff-70f4-4383-bfce-a818f698ab5b",
   "metadata": {},
   "source": [
    "### Step 5: In SparkSQL it is also possible to create *User-Defined Functions (UDF)* to use within queries\n",
    "1. See how this can be accomplished in [this page](https://sparkbyexamples.com/pyspark/pyspark-udf-user-defined-function/)\n",
    "2. Show an example using the dataframes loaded in this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a6a7b5-4908-4849-a097-7590911e8331",
   "metadata": {},
   "outputs": [],
   "source": [
    "# .. TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b9a1fd-71b6-4a52-b425-333036a64ecf",
   "metadata": {},
   "source": [
    "### Step 6: Take two query examples from the previous steps and `explain` their execution plans\n",
    "\n",
    "*Syntax:* `df.explain(True)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b5a3991-614a-46e8-8356-92d631a4098d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# .. TODO"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
