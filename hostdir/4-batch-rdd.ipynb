{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2665f558-53d8-4586-bfa4-43830aa5384f",
   "metadata": {},
   "source": [
    "# EX4-BATCH: Introduction to Spark programming\n",
    "\n",
    "Your assignment: complete the `TODO`'s and include also the **output of each cell**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc7fe56c-bea1-4cfa-b942-b9c9ddfa040b",
   "metadata": {},
   "source": [
    "### Step 1: First Spark program\n",
    "\n",
    "Important observations:\n",
    "\n",
    "- This code corresponds to the **driver program** and hence, it must connect to a **Spark master**.\n",
    "- You have some options to the Spark Master: in lab we should have a Spark service available at `spark://spark:7077`. But you can also run this code locally with `local[*]` as the master parameter -- this means that driver and executors are going to be deployed in you machine. *Program once, run everywhere*.\n",
    "- The SparkContext is started and stopped in the cell below -- this means that the application will finish after. If you need to keep the application alive, do not call `sc.stop()` until finished."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1f50ba8f-805e-45ec-abba-a08281baa4e4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum of RDD1:  15\n",
      "Average of RDD1:  3.0\n",
      "one : 1\n",
      "two : 1\n",
      "three : 1\n"
     ]
    }
   ],
   "source": [
    "from pyspark.context import SparkContext\n",
    "\n",
    "try:\n",
    "    sc.stop()\n",
    "except NameError:\n",
    "    print(\"SparkContext not defined\")    \n",
    "\n",
    "# ---- Local execution\n",
    "sc = SparkContext(appName=\"SparkStarters\", master=\"local[*]\")\n",
    "\n",
    "# ---- Distributed execution\n",
    "# sc = SparkContext(appName=\"SparkStarters\", master=\"spark://spark:7077\")\n",
    "\n",
    "rdd1 = sc.parallelize([1, 2, 3, 4, 5])\n",
    "rdd2 = sc.parallelize(['one', 'two', 'three'])\n",
    "\n",
    "sum_rdd1 = rdd1.sum()\n",
    "avg_rdd1 = rdd1.mean()\n",
    "\n",
    "print(\"Sum of RDD1: \", sum_rdd1)\n",
    "print(\"Average of RDD1: \", avg_rdd1)\n",
    "\n",
    "word_count = rdd2.countByValue()\n",
    "for word, count in word_count.items():\n",
    "    print(word, \":\", count)\n",
    "\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef1b1c8c-bc1f-4ca6-be7c-e269db1804b7",
   "metadata": {},
   "source": [
    "### Step 2: Quick start tutorial on Spark RDD API `#TODO`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "761a298c-e02b-4ef2-9b6f-03980193d120",
   "metadata": {},
   "source": [
    "1. Complete the [SparkByExample RDD tutorial](https://sparkbyexamples.com/pyspark-rdd/).\n",
    "2. Include here in this notebook (add cells to it): the commands, the explanation of each cell, and the output of each command.\n",
    "3. In the parts where an input file is required, use some data from the previous classes (e.g. the HDFS log dataset)\n",
    "4. Take this opportunity to try the operators discussed in class. Also, web UI for inspecting the application running time."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
