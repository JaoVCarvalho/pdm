{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2665f558-53d8-4586-bfa4-43830aa5384f",
   "metadata": {},
   "source": [
    "# EX8-STREAM: Spark Structured Streaming basics\n",
    "\n",
    "Your assignment: complete the `TODO`'s and include also the **output of each cell**.\n",
    "\n",
    "#### You may need to read the [Structured Streaming API Documentation](https://spark.apache.org/docs/latest/api/python/reference/pyspark.ss/index.html) to complete this lab."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d78793a7-6d38-4794-a7d1-6b2c0e57ca8f",
   "metadata": {},
   "source": [
    "### Step 2: Start Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6991433e-339e-4dde-a08d-d31ce0c45a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "try:\n",
    "    spark.stop()\n",
    "except NameError:\n",
    "    print(\"SparkContext not defined\")\n",
    "\n",
    "# local mode\n",
    "spark = SparkSession.builder \\\n",
    "            .appName(\"Spark SQL basic example\") \\\n",
    "            .master(\"local[*]\") \\\n",
    "\t    \t.config(\"spark.some.config.option\", \"some-value\") \\\n",
    "\t    \t.getOrCreate()\n",
    "\n",
    "# cluster mode\n",
    "#spark = SparkSession.builder \\\n",
    "#            .appName(\"Spark SQL basic example\") \\\n",
    "#            .master(\"spark://spark:7077\") \\\n",
    "#\t    \t.config(\"spark.some.config.option\", \"some-value\") \\\n",
    "#\t    \t.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "805b2706-6fd2-4bb3-bef1-059c94a66a80",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/21 22:18:39 WARN TextSocketSourceProvider: The socket source should not be used for production applications! It does not support recovery.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[value: string]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines = (spark\n",
    ".readStream.format(\"socket\")\n",
    ".option(\"host\", \"localhost\")\n",
    ".option(\"port\", 9999)\n",
    ".load())\n",
    "\n",
    "lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "44b2f66c-8e0c-4ece-9640-1688d4f2982a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[key: string, count: bigint]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "#words = lines.select(explode(split(lines.value, \"\\\\s\")).alias(\"word\"))\n",
    "words = lines.select(split(lines.value, \"\\\\s\").alias(\"key_value\"))\n",
    "words = words.select(words.key_value.getItem(0).alias(\"key\"), words.key_value.getItem(1).alias(\"value\"))\n",
    "counts = words.groupBy(\"key\").count()\n",
    "counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dac43346-7f95-4681-aea6-f1b929bc424e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.streaming.readwriter.DataStreamWriter at 0x7e57c22ee0c0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "writer = counts.writeStream.format(\"console\").outputMode(\"complete\")\n",
    "writer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7aaf7ad4-75ec-4a9a-b77a-e0b303508f89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.streaming.readwriter.DataStreamWriter at 0x7e57c22ee0c0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpointDir = \"/tmp/structured_streaming\"\n",
    "writer2 = (writer\n",
    ".trigger(processingTime=\"5 second\")\n",
    ".option(\"checkpointLocation\", checkpointDir)\n",
    "          )\n",
    "writer2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85d6b865-d688-4877-91c0-315f57107fe2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/21 22:18:40 ERROR MicroBatchExecution: Query [id = be0c67dd-8c73-4f9b-961b-5f2d32b3697d, runId = 9135dd5f-800e-405e-8a04-daee1c90a83f] terminated with error\n",
      "java.io.FileNotFoundException: File file:/tmp/structured_streaming/offsets does not exist\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.getFileStatus(DelegateToFileSystem.java:128)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.createInternal(DelegateToFileSystem.java:93)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs$ChecksumFSOutputSummer.<init>(ChecksumFs.java:353)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.createInternal(ChecksumFs.java:400)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.create(AbstractFileSystem.java:626)\n",
      "\tat org.apache.hadoop.fs.FileContext$3.next(FileContext.java:701)\n",
      "\tat org.apache.hadoop.fs.FileContext$3.next(FileContext.java:697)\n",
      "\tat org.apache.hadoop.fs.FSLinkResolver.resolve(FSLinkResolver.java:90)\n",
      "\tat org.apache.hadoop.fs.FileContext.create(FileContext.java:703)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.createTempFile(CheckpointFileManager.scala:361)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.<init>(CheckpointFileManager.scala:140)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.<init>(CheckpointFileManager.scala:143)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.createAtomic(CheckpointFileManager.scala:367)\n",
      "\tat org.apache.spark.sql.execution.streaming.HDFSMetadataLog.write(HDFSMetadataLog.scala:201)\n",
      "\tat org.apache.spark.sql.execution.streaming.HDFSMetadataLog.addNewBatchByStream(HDFSMetadataLog.scala:237)\n",
      "\tat org.apache.spark.sql.execution.streaming.HDFSMetadataLog.add(HDFSMetadataLog.scala:130)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.markMicroBatchStart(MicroBatchExecution.scala:764)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$13(MicroBatchExecution.scala:536)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:535)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "25/03/21 22:18:40 WARN NettyRpcEnv: Ignored failure: java.util.concurrent.RejectedExecutionException: Task java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask@515dbea5[Not completed, task = java.util.concurrent.Executors$RunnableAdapter@68ce254[Wrapped task = org.apache.spark.rpc.netty.NettyRpcEnv$$anon$1@11deb00b]] rejected from java.util.concurrent.ScheduledThreadPoolExecutor@65bb0aba[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 0]\n",
      "Exception in thread \"stream execution thread for [id = be0c67dd-8c73-4f9b-961b-5f2d32b3697d, runId = 9135dd5f-800e-405e-8a04-daee1c90a83f]\" org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef.deactivateInstances(StateStoreCoordinator.scala:119)\n",
      "\tat org.apache.spark.sql.streaming.StreamingQueryManager.notifyQueryTermination(StreamingQueryManager.scala:426)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$3(StreamExecution.scala:360)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:340)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "Caused by: org.apache.spark.rpc.RpcEnvStoppedException: RpcEnv already stopped.\n",
      "\tat org.apache.spark.rpc.netty.Dispatcher.postMessage(Dispatcher.scala:176)\n",
      "\tat org.apache.spark.rpc.netty.Dispatcher.postLocalMessage(Dispatcher.scala:144)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.askAbortable(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEndpointRef.askAbortable(NettyRpcEnv.scala:554)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEndpointRef.ask(NettyRpcEnv.scala:558)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:100)\n",
      "\t... 11 more\n",
      "25/03/21 22:18:40 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+---+-----+\n",
      "|key|count|\n",
      "+---+-----+\n",
      "+---+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 1\n",
      "-------------------------------------------\n",
      "+-----+-----+\n",
      "|  key|count|\n",
      "+-----+-----+\n",
      "|Hello|    5|\n",
      "+-----+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 2\n",
      "-------------------------------------------\n",
      "+-----+-----+\n",
      "|  key|count|\n",
      "+-----+-----+\n",
      "|Hello|   10|\n",
      "+-----+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 3\n",
      "-------------------------------------------\n",
      "+-----+-----+\n",
      "|  key|count|\n",
      "+-----+-----+\n",
      "|Hello|   15|\n",
      "+-----+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/21 22:19:00 WARN TextSocketMicroBatchStream: Stream closed by localhost:9999\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 4\n",
      "-------------------------------------------\n",
      "+-----+-----+\n",
      "|  key|count|\n",
      "+-----+-----+\n",
      "|Hello|   20|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!rm -r /tmp/structured_streaming/\n",
    "streamingQuery = writer2.start()\n",
    "streamingQuery.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e9efc8-b1f6-4ca5-b524-98c1bfc3a111",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
