{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2665f558-53d8-4586-bfa4-43830aa5384f",
   "metadata": {},
   "source": [
    "# EX8-STREAM: Spark Structured Streaming\n",
    "\n",
    "Your assignment: complete the `TODO`'s and include also the **output of each cell**.\n",
    "\n",
    "#### You may need to read the [Structured Streaming API Documentation](https://spark.apache.org/docs/latest/api/python/reference/pyspark.ss/index.html) to complete this lab."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d78793a7-6d38-4794-a7d1-6b2c0e57ca8f",
   "metadata": {},
   "source": [
    "### Step 1: **[PLAN A]** Start Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6991433e-339e-4dde-a08d-d31ce0c45a54",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "try:\n",
    "    spark.stop()\n",
    "except NameError:\n",
    "    print(\"SparkContext not defined\")\n",
    "\n",
    "    # cluster mode\n",
    "spark = SparkSession.builder \\\n",
    "            .appName(\"Spark SQL basic example\") \\\n",
    "            .master(\"spark://spark:7077\") \\\n",
    "\t    \t.config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:3.3.4\") \\\n",
    "            .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://minio:9000\") \\\n",
    "            .config(\"spark.hadoop.fs.s3a.access.key\", \"pdm_minio\") \\\n",
    "            .config(\"spark.hadoop.fs.s3a.secret.key\", \"pdm_minio\") \\\n",
    "            .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
    "            .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "            .config(\"spark.hadoop.fs.s3a.connection.ssl.enabled\", \"false\") \\\n",
    "\t    \t.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8a733c0-38b0-434e-adba-458de9f0ea06",
   "metadata": {},
   "source": [
    "### Step 1: **[PLAN B]** Start Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22172b02-7b73-4dec-befd-fa92a9f364e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "try:\n",
    "    spark.stop()\n",
    "except NameError:\n",
    "    print(\"SparkContext not defined\")\n",
    "    \n",
    "\n",
    "# local mode\n",
    "spark = SparkSession.builder \\\n",
    "            .appName(\"Spark SQL basic example\") \\\n",
    "            .master(\"local[*]\") \\\n",
    "\t    \t.config(\"spark.some.config.option\", \"some-value\") \\\n",
    "\t    \t.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf5215e2-5002-44e1-ad23-1b65803b09ee",
   "metadata": {},
   "source": [
    "### Step 2: Static Dataframe of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e328c85-b3a5-4746-9f9e-7caf491c5075",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "words_df = spark.read.csv(\"s3a://public/100words.txt.gz\") # plan A\n",
    "#words_df = spark.read.csv(\"data/100words.txt.gz\") # plan B\n",
    "words_df = words_df.withColumnRenamed(\"_c0\", \"word\")\n",
    "words_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca1f7aeb-2b85-40a0-9932-2e086ff4852b",
   "metadata": {},
   "source": [
    "### Step 3: Get meaning for each word (use [Free Dictionary API](https://dictionaryapi.dev/))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db372c8-9818-46b9-8737-a4b424886522",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "import requests_ratelimiter\n",
    "\n",
    "def get_word_meaning(word, session):\n",
    "    url = f\"https://api.dictionaryapi.dev/api/v2/entries/en/{word}\"\n",
    "    response = session.get(url)\n",
    "    response.raise_for_status()  # Ensure the request was successful\n",
    "    json_data = response.json()\n",
    "\n",
    "    try:\n",
    "        meaning = json_data[0]['meanings'][0]['definitions'][0]['definition']\n",
    "    except:\n",
    "        meaning = \"__NOT_FOUND__\"\n",
    "\n",
    "    return meaning\n",
    "\n",
    "\n",
    "try:\n",
    "    words_with_meaning_df.cache()\n",
    "    words_with_meaning_df.show()\n",
    "except NameError:\n",
    "    print(\"words_with_meaning_df not defined\")\n",
    "    meanings = []\n",
    "    session = requests_ratelimiter.LimiterSession(per_second=1)\n",
    "    for word in [r.word for r in words_df.collect()]:\n",
    "        meanings.append((word, get_word_meaning(word, session)))\n",
    "        print(word)\n",
    "    words_with_meaning_df = spark.createDataFrame(meanings, [\"word\", \"meaning\"])\n",
    "    words_with_meaning_df.cache()\n",
    "    words_with_meaning_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75e3bea3-78c9-4ca4-a749-d6fd3632fd1a",
   "metadata": {},
   "source": [
    "### Step 4: **[PLAN A]** Create a stream of sentences using existing socket stream (LAB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "805b2706-6fd2-4bb3-bef1-059c94a66a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "words_stream = spark \\\n",
    "    .readStream.format(\"socket\") \\\n",
    "    .option(\"host\", \"socketstreamserver\") \\\n",
    "    .option(\"port\", 12345) \\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "142297a7-1ad8-4ef8-99fb-19f97df651c4",
   "metadata": {},
   "source": [
    "### Step 4: **[PLAN B]** Create a socket stream and create a stream of sentences from that (NOTEBOOK LOCAL)\n",
    "\n",
    "1. Before running the cell below, start socket stream from existing script `hostdir/bin/cmd.sh` using a notebook terminal.\n",
    "2. Make sure it is running properly.\n",
    "3. Create a spark stream using the command below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba24112f-12a6-4249-9968-c6ce5b71e093",
   "metadata": {},
   "outputs": [],
   "source": [
    "words_stream = spark \\\n",
    "    .readStream.format(\"socket\") \\\n",
    "    .option(\"host\", \"localhost\") \\\n",
    "    .option(\"port\", 12345) \\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a10542c-9305-4335-95e4-efa3ea017564",
   "metadata": {},
   "source": [
    "### Step 5: Start stream just to visualize some of its values (for 10 seconds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac43346-7f95-4681-aea6-f1b929bc424e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 42\n",
      "-------------------------------------------\n",
      "+--------------------+\n",
      "|               value|\n",
      "+--------------------+\n",
      "|18 vacation park ...|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "words_stream_writer = words_stream.writeStream.format(\"console\").outputMode(\"append\")\n",
    "words_stream_writer = words_stream_writer.trigger(processingTime=\"1 second\")\n",
    "words_stream_query = words_stream_writer.start()\n",
    "words_stream_query.awaitTermination(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bcf4016-2e1a-446c-99c6-855d57848385",
   "metadata": {},
   "source": [
    "### Step 6: Transform the stream as requested `#TODO`\n",
    "\n",
    "1. Each line of the stream starts with a number, let us call this number `user_id`. The rest of the line comprises a set of words generated by this user.\n",
    "2. For each user request you must take the corresponding words, get the meaning of each word (static dataframe) and return the responses as a new stream of `user_id, [<meaning of word 1>, <meaning of word 2>, ... ]`\n",
    "3. Let the stream running on console for 10 seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e9efc8-b1f6-4ca5-b524-98c1bfc3a111",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code here: TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3550aa0-c822-4bf2-98e4-9f5a5708e6ae",
   "metadata": {},
   "source": [
    "### Step 7: Transform the stream as requested `#TODO`\n",
    "\n",
    "1. Again, from the stream of lines `words_stream`\n",
    "2. Map each line to rows of `word,user_id` (hint: use `explode` and `split`)\n",
    "3. From this new stream, group by word and aggregate the set of user IDs that asked for that specific word.\n",
    "4. Generate a stream of `<list of user IDs> <word> <meaning of word>`\n",
    "5. Let the resulting stream running for 20 seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe3dd803-d53b-46fe-beec-5e26b4913a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code here: TODO"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
